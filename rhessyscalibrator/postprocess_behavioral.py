"""@package rhessyscalibrator.postprocess_behavioral

@brief Tool for analyzing model run results generated by rhessys_calibrator_behavioral.py and 
stored in a database format managed by rhessyscalibrator.model_runner_db.py.  Currently
plots 95% uncertainty bounds around observed streamflow using NSE as a likelihood function.

This software is provided free of charge under the New BSD License. Please see
the following license information:

Copyright (c) 2013, University of North Carolina at Chapel Hill
All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above copyright
      notice, this list of conditions and the following disclaimer in the
      documentation and/or other materials provided with the distribution.
    * Neither the name of the University of North Carolina at Chapel Hill nor the
      names of its contributors may be used to endorse or promote products
      derived from this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE UNIVERSITY OF NORTH CAROLINA AT CHAPEL HILL
BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR 
CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE
GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT 
LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT
OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.


@author Brian Miles <brian_miles@unc.edu>
"""
import os, sys, errno
import argparse
import logging
from datetime import datetime

import math
import numpy as np
import pandas as pd
from scipy import stats
import statsmodels.api as sm
import matplotlib.pyplot as plt
import matplotlib
from matplotlib.ticker import FuncFormatter

from rhessysworkflows.rhessys import RHESSysOutput

from rhessyscalibrator.calibrator import RHESSysCalibrator
from rhessyscalibrator.model_runner_db import *


def to_percent(y, position):
    # Ignore the passed in position. This has the effect of scaling the default
    # tick locations.
    s = "%.0f" % ( (100 * y), )

    # The percent symbol needs escaping in latex
    if matplotlib.rcParams['text.usetex'] == True:
        return s + r'$\%$'
    else:
        return s + '%'


def calculateUncertaintyBounds(ysim, likelihood, lowerBound, upperBound):
    """ Calculate uncertainty bounds for matrix of simulated values
    
        @param ysim Numpy array containing a vector of data for a number of simuations,
          dimensions [NUM_SIMULATIONS, NUM_DATA_PER_SIMULATION]
        @param likelihood Numpy array containing model fitness parameter for each
          simulation, dimensions [NUM_SIMULATIONS]
        @param lowerBound Double representing percentile of lower bound of confidence interval (e.g. 2.5)
        @param upperBound Double representing percentile of upper bound of confidence interval (e.g. 97.5)
        
        @return Tuple of numpy arrays representing minYsim, maxYsim, and medianYsim, dimensions
          [NUM_DATA_PER_SIMULATION]  
    """
    assert( lowerBound < 100.0 and lowerBound > 0.0)
    assert( upperBound < 100.0 and upperBound > 0.0)
    assert( lowerBound < upperBound )
    
    # Normalize likelihood to have values from 0 to 1
    normLH = likelihood / np.sum(likelihood)
    
    lower = lowerBound / 100.0
    upper = upperBound / 100.0

    nIters = np.shape(ysim)[1]
    minYsim = np.zeros(nIters)
    maxYsim = np.zeros(nIters)
    medianYsim = np.zeros(nIters)
    meanYsim = np.zeros(nIters)

    # Generate uncertainty interval bounded by lower bound and upper bound
    for i in xrange(0, nIters):
        ys = ysim[:,i]
        # Use CDF of likelihood values as basis for interval
        sortedIdx = np.argsort(ys)
        sortYsim = ys[sortedIdx]
        sortLH = normLH[sortedIdx]
        cumLH = np.cumsum(sortLH)
        cond = (cumLH > lower) & (cumLH < upper)
            
        validYsim = sortYsim[cond]
        minYsim[i] = validYsim[0]
        maxYsim[i] = validYsim[-1]
        medianYsim[i] = np.median(validYsim)
        meanYsim[i] = np.mean(validYsim)
        
    return (minYsim, maxYsim, medianYsim, meanYsim)


class RHESSysCalibratorPostprocessBehavioral(object):
    """ Main driver class for rhessys_calibrator_postprocess_behavioral tool
    """
    
    def saveUncertaintyBoundsPlot(self, outDir, filename, lowerBound, upperBound,
                                  format='PDF', log=False, xlabel=None, ylabel=None,
                                  title=None, plotObs=True, plotMedian=False, plotMean=False, 
                                  plotColor=False, legend=True, sizeX=1, sizeY=1, dpi=80):
        """ Save uncertainty bounds plot to outDir
        
            @param lowerBound Float <100.0, >0.0, <upperBound
            @param upperBound Float <100.0, >0.0, >lowerBound
        """
        assert( format in ['PDF', 'PNG'] )
        if format == 'PDF':
            plotFilename = "%s.pdf" % (filename,)
        elif format == 'PNG':
            plotFilename = "%s.png" % (filename,)
        plotFilepath = os.path.join(outDir, plotFilename)
        
        assert( self.x is not None )
        assert( self.ysim is not None )
        
        if plotColor:
            fillColor = '0.5'
            obs_color = 'blue'
            median_color = 'magenta'
            mean_color = 'green'
            min_color = 'grey'
            max_color = 'grey'
        else:
            fillColor = '0.5'
            obs_color = 'black'
            median_color = '0.75'
            mean_color = '0.25'
            min_color = 'grey'
            max_color = 'grey'
        
        # Get the uncertainty boundary
        (minYsim, maxYsim, medianYsim, meanYsim) = \
            calculateUncertaintyBounds(self.ysim, self.likelihood,
                                       lowerBound, upperBound)
            
        # Percent observations within uncertainty bounds
        numObs = len(self.obs)
        print("\nNumber of observations: %d" % (numObs,) )
        obsLteMax = np.where(self.obs <= maxYsim)[0]
        obsGteMin = np.where(self.obs >= minYsim)[0]
        obsInBounds = np.intersect1d(obsLteMax, obsGteMin)
        numObsInBounds = len(obsInBounds)
        pctObsInBounds = ( float( numObsInBounds) / float( numObs ) ) * 100.0
        print("Observations within prediction bounds: %d (%.2f%%)\n" % \
              (numObsInBounds, pctObsInBounds) )
        
        # Plot it up
        fig = plt.figure(figsize=(sizeX, sizeY), dpi=dpi, tight_layout=True)
        ax = fig.add_subplot(121)
        
        data_plt = []
        legend_items = []
        # Draw observed line
        if plotObs:
            (p, ) = ax.plot(self.x, self.obs, color=obs_color, linestyle='solid')
            data_plt.append(p)
            legend_items.append('Observed data')
        if plotMedian:
            (p, ) = ax.plot(self.x, medianYsim, color=median_color, linestyle='solid')
            data_plt.append(p)
            legend_items.append('Median simulated')
        if plotMean:
            (p, ) = ax.plot(self.x, meanYsim, color=mean_color, linestyle='solid')
            data_plt.append(p)
            legend_items.append('Mean simulated')
        # Draw shaded uncertainty envelope
        ax.fill_between(self.x, minYsim, maxYsim, color=fillColor)
        
        # Annotations
        # X-axis
        quarterly = matplotlib.dates.MonthLocator(interval=3)
        ax.xaxis.set_major_locator(quarterly)
        ax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter('%b-%Y') )
        # Rotate
        plt.setp( ax.xaxis.get_majorticklabels(), rotation=45 )
        plt.setp( ax.xaxis.get_majorticklabels(), fontsize=6 )
        
        # Y-axis
        plt.setp( ax.get_yticklabels(), fontsize=6 )
        
        if log:
            ax.set_yscale('log')
        
        if xlabel:
            ax.set_xlabel(xlabel)
        if ylabel:
            ax.set_ylabel(ylabel)
        if title:
            fig.suptitle(title, y=1.03)
        
        # Plot exceedance plot
        ax2 = fig.add_subplot(122)
        
        min_x = np.min(self.ysim)
        min_obs = np.min(self.obs)
        min_x = min(min_x, min_obs)
        max_x = np.max(self.ysim)
        max_obs = np.max(self.obs)
        max_x = max(max_x, max_obs)
        x = np.linspace(min_x, max_x, num=len(self.obs) )
        
        obs_plt = None
        if plotObs:
            obs_ecdf = sm.distributions.ECDF(self.obs)
            obs_y = 1 - obs_ecdf(x)
            ax2.plot(obs_y, x, color=obs_color)
        if plotMedian:
            med_ecdf = sm.distributions.ECDF(medianYsim)
            med_y = 1 - med_ecdf(x)
            ax2.plot(med_y, x, color=median_color, linestyle='solid')
        if plotMean:
            mean_ecdf = sm.distributions.ECDF(meanYsim)
            mean_y = 1 - mean_ecdf(x)
            ax2.plot(mean_y, x, color=mean_color, linestyle='solid')
        
        min_ecdf = sm.distributions.ECDF(minYsim)
        min_y = 1 - min_ecdf(x)
        ax2.plot(min_y, x, color=min_color, linestyle='dashed')
        
        max_ecdf = sm.distributions.ECDF(maxYsim)
        max_y = 1 - max_ecdf(x)
        ax2.plot(max_y, x, color=max_color, linestyle='dashed')
        
        # Annotations
        # X-axis
        formatter = FuncFormatter(to_percent)
        ax2.xaxis.set_major_formatter(formatter)
        ax2.set_xlabel('Exceedance probability (%)')
        
        # Y-axis
        if log:
            ax2.set_yscale('log')
        else:
            ax2.set_ylim( ax.get_ylim() )
        
        plt.setp( ax2.get_xticklabels(), fontsize=6 )
        plt.setp( ax2.get_yticklabels(), fontsize=6 )
        
        # Plot legend last
        if legend:
            legend = fig.legend( data_plt, legend_items, 'lower center', fontsize='x-small', 
                                ncol=len(legend_items), frameon=True )
            frame = legend.get_frame()
            frame.set_facecolor('0.9')
            frame.set_alpha(0.5)
        
        # Save the figure
        fig.savefig(plotFilepath, format=format, bbox_inches='tight', pad_inches=0.25)
    
    def _initLogger(self, level):
        """ Setup logger.  Log to the console for now """
        self.logger = logging.getLogger("cluster_calibrator")
        self.logger.setLevel(level)
        # console handler and set it to level
        consoleHandler = logging.StreamHandler()
        consoleHandler.setLevel(level)
        # create formatter
        formatter = \
            logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
        # add formatter to console handler
        consoleHandler.setFormatter(formatter)
        # add consoleHandler to logger
        self.logger.addHandler(consoleHandler)
        
        
    def readBehavioralData(self, basedir, session_id, variable='streamflow',
                           observed_file=None, behavioral_filter=None):
        
        dbPath = RHESSysCalibrator.getDBPath(basedir)
        if not os.access(dbPath, os.R_OK):
            raise IOError(errno.EACCES, "The database at %s is not readable" %
                          dbPath)
        self.logger.debug("DB path: %s" % dbPath)
        
        outputPath = RHESSysCalibrator.getOutputPath(basedir)
        if not os.access(outputPath, os.R_OK):
            raise IOError(errno.EACCES, "The output directory %s is  not readable" % outputPath)
        self.logger.debug("Output path: %s" % outputPath)

        rhessysPath = RHESSysCalibrator.getRhessysPath(basedir)
        
        calibratorDB = \
            ModelRunnerDB(RHESSysCalibrator.getDBPath(
                basedir))
        
        # Make sure the session exists
        session = calibratorDB.getSession(session_id)
        if None == session:
            raise Exception("Session %d was not found in the calibration database %s" % (session_id, dbPath))
        if session.status != "complete":
            print "WARNING: session status is: %s.  Some model runs may not have completed." % (session.status,)
        else:
            self.logger.debug("Session status is: %s" % (session.status,))
        
        # Determine observation file path
        if observed_file:
            obs_file = observed_file
        else:
            # Get observered file from session
            assert( session.obs_filename != None )
            obs_file = session.obs_filename
        obsPath = RHESSysCalibrator.getObsPath(basedir)
        obsFilePath = os.path.join(obsPath, obs_file)
        if not os.access(obsFilePath, os.R_OK):
            raise IOError(errno.EACCES, "The observed data file %s is  not readable" % obsFilePath)
        self.logger.debug("Obs path: %s" % obsFilePath)
        
        # Get runs in session
        runs = calibratorDB.getRunsInSession(session.id, where_clause=behavioral_filter)
        numRuns = len(runs) 
        if numRuns == 0:
            raise Exception("No runs found for session %d" 
                            % (session.id,))
        response = raw_input("%d runs selected for plotting from session %d in basedir '%s', continue? [yes | no] " % \
                            (numRuns, session_id, os.path.basename(basedir) ) )
        response = response.lower()
        if response != 'y' and response != 'yes':
            # Exit normally
            return 0
        self.logger.debug("%d behavioral runs" % (numRuns,) )
        
        # Read observed data from file
        obsFile = open(obsFilePath, 'r')
        (obs_datetime, obs_data) = \
            RHESSysOutput.readObservedDataFromFile(obsFile)
        obsFile.close()
        obs = pd.Series(obs_data, index=obs_datetime)
        
        self.logger.debug("Observed data: %s" % obs_data)
        
        likelihood = np.empty(numRuns)
        ysim = None
        x = None
        
        runsProcessed = False
        for (i, run) in enumerate(runs):
            if "DONE" == run.status:
                runOutput = os.path.join(rhessysPath, run.output_path)
                self.logger.debug(">>>\nOutput dir of run %d is %s" %
                                     (run.id, runOutput))
                tmpOutfile = \
                    RHESSysCalibrator.getRunOutputFilePath(runOutput)
                if not os.access(tmpOutfile, os.R_OK):
                    print "Output file %s for run %d not found or not readable, unable to calculate fitness statistics for this run" % (tmpOutfile, run.id)
                    continue
                
                tmpFile = open(tmpOutfile, 'r')
                
                (tmp_datetime, tmp_data) = \
                        RHESSysOutput.readColumnFromFile(tmpFile,
                                                         "streamflow")
                tmp_mod = pd.Series(tmp_data, index=tmp_datetime)
                # Align timeseries to observed
                (mod, obs) = tmp_mod.align(obs, join='inner')
                                 
                # Stash date for X values (assume they are the same for all runs
                if x == None:
                    x = [datetime.strptime(str(d), '%Y-%m-%d %H:%M:%S') for d in mod.index]
        
                # Put data in matrix
                dataLen = len(mod)
                if ysim == None:
                    # Allocate matrix for results
                    ysim = np.empty( (numRuns, dataLen) )
                assert( np.shape(ysim)[1] == dataLen )
                ysim[i,] = mod
                
                # Store fitness parameter
                likelihood[i] = run.nse
                        
                tmpFile.close()                
                runsProcessed = True
        
        return (runsProcessed, obs, x, ysim, likelihood)
    
    
    def main(self, args):
        # Set up command line options
        parser = argparse.ArgumentParser(description="Tool for visualizing output from behavioral model runs for RHESSys")
        parser.add_argument("-b", "--basedir", action="store", 
                            dest="basedir", required=True,
                            help="Base directory for the calibration session")
        
        parser.add_argument("-s", "--behavioral_session", action="store", type=int,
                            dest="session_id", required=True,
                            help="Session to use for behavioral runs.")
        
        parser.add_argument("-t", "--title", action="store",
                            dest="title",
                            help="Title to add to output figures")
        
        parser.add_argument("-o", "--outdir", action="store",
                            dest="outdir",
                            help="Output directory in which to place figures.  If not specified, basedir will be used.")

        parser.add_argument("-f", "--file", action="store",
                            dest="observed_file",
                            help="The name of the observed file to use for calculating model fitness statistics.  Filename will be interpreted as being relative to $BASEDIR/obs. If not supplied observation file from calibration model run will be used.")

        parser.add_argument("-of", "--outputFormat", action="store",
                            dest="outputFormat", default="PDF", choices=["PDF", "PNG"],
                            help="Output format to save figures in.")

        parser.add_argument("--behavioral_filter", action="store",
                            dest="behavioral_filter", required=False,
                            default="nse>0.5 and nse_log>0.5",
                            help="SQL where clause to use to determine which runs qualify as behavioral parameters.  E.g. 'nse>0.5 AND nse_log>0.5' (use quotes)")

        parser.add_argument("--supressObs", action="store_true", required=False, default=False,
                            help="Do not plot observered data")

        parser.add_argument("--plotMedian", action="store_true", required=False, default=False,
                            help="Plot median value of behavioral runs")
        
        parser.add_argument("--plotMean", action="store_true", required=False, default=False,
                            help="Plot mean value of behavioral runs")

        parser.add_argument("--color", action="store_true", required=False, default=False,
                            help="Plot in color")
        
        parser.add_argument("--legend", action="store_true", required=False, default=False,
                            help="Show legend")
        
        parser.add_argument('--figureX', required=False, type=int, default=4,
                            help='The width of the plot, in inches')
    
        parser.add_argument('--figureY', required=False, type=int, default=3,
                            help='The height of the plot, in inches')

        parser.add_argument("-l", "--loglevel", action="store",
                            dest="loglevel", default="OFF",
                            help="Set logging level, one of: OFF [default], DEBUG, CRITICAL (case sensitive)")
        
        options = parser.parse_args()
        
        # Enforce initial command line options rules
        if "DEBUG" == options.loglevel:
            self._initLogger(logging.DEBUG)
        elif "CRITICAL" == options.loglevel:
            self._initLogger(logging.CRITICAL)
        else:
            self._initLogger(logging.NOTSET)
        
        if not os.path.isdir(options.basedir) or not os.access(options.basedir, os.W_OK):
            sys.exit("Unable to write to basedir %s" % (options.basedir,) )
        basedir = os.path.abspath(options.basedir)
        
        if not options.outdir:
            options.outdir = basedir
            
        if not os.path.isdir(options.outdir) and os.access(options.outdir, os.W_OK):
            parser.error("Figure output directory %s must be a writable directory" % (options.outdir,) )
        outdirPath = os.path.abspath(options.outdir)

        try:
            (runsProcessed, self.obs, self.x, self.ysim, self.likelihood) = \
                self.readBehavioralData(basedir, options.session_id, 'streamflow',
                                        options.observed_file, options.behavioral_filter)
            
            if runsProcessed:
                behavioralFilename = 'behavioral'
                if options.supressObs:
                    behavioralFilename += '_noObs'
                if options.plotMedian:
                    behavioralFilename += '_median'
                if options.plotMean:
                    behavioralFilename += '_mean'
                if options.color:
                    behavioralFilename += '_color'
                if options.legend:
                    behavioralFilename += '_legend'
                behavioralFilename += "_SESSION_%s" % ( options.session_id, )
                # Generate visualizations
                self.saveUncertaintyBoundsPlot(outdirPath, behavioralFilename, 
                                               2.5, 97.5, format=options.outputFormat, log=False,
                                               ylabel=r'Streamflow ($mm^{-d}$)',
                                               title=options.title, 
                                               plotObs=(not options.supressObs),
                                               plotMedian=options.plotMedian,
                                               plotMean=options.plotMean,
                                               plotColor=options.color,
                                               legend=options.legend,
                                               sizeX=options.figureX, sizeY=options.figureY )
                behavioralFilename += '-log'
                self.saveUncertaintyBoundsPlot(outdirPath, behavioralFilename, 
                                               2.5, 97.5, format=options.outputFormat, log=True,
                                               ylabel=r'Streamflow ($mm^{-d}$)',
                                               title=options.title,
                                               plotObs=(not options.supressObs),
                                               plotMedian=options.plotMedian,
                                               plotMean=options.plotMean,
                                               plotColor=options.color,
                                               legend=options.legend,
                                               sizeX=options.figureX, sizeY=options.figureY )
        except:
            raise
        else:
            self.logger.debug("exiting normally")
            return 0
        finally:
            # Decrement reference count, this will (hopefully) allow __del__
            #  to be called on the once referenced object
            calibratorDB = None
            
class BehavioralComparison(RHESSysCalibratorPostprocessBehavioral):
    
    def calculateKolmogorovSmirnov(self, data1, data2):
        """ Test whether the behavioral data are from the same distribution using 
            Kolmogorov-Smirnov statistic
            More information: http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ks_2samp.html
                              http://en.wikipedia.org/wiki/Kolmogorov-Smirnov_test
                              http://www.soest.hawaii.edu/wessel/courses/gg313/Critical_KS.pdf
            @param data1 Array of sample data assumed to be drawn from a continuous distribution
            @param data2 Array of sample data assumed to be drawn from a continuous distribution
        """
        # Calculate critical value of K-S statistic D
        ks_crit_table = { 0.10: 1.22, 0.05: 1.36, 0.025: 1.48, 0.01: 1.63, 0.005: 1.73, 0.001: 1.95 }
        #import pdb; pdb.set_trace()
        alpha = 0.05
        c_alpha = ks_crit_table[alpha]
        n1 = len(data1)
        n2 = len(data2)
        ks_crit = c_alpha * math.sqrt( float(n1 + n2) / float(n1 * n2) )
        print("\nCritical value for Kolmogorov-Smirnov statistic (D_alpha; alpha=%.4f): %.4f" % (alpha, ks_crit) )
        
        # Calculate K-S statistic D
        (ks_statistic, p_value) = stats.ks_2samp(data1, data2)
        print("Kolmogorov-Smirnov statistic (D): %.4f" % (ks_statistic,) )
        print("D > D_alpha? %s" % ( (ks_statistic > ks_crit), ) )
        print("\np-value: %.4f" % (p_value,) )
        print("p-value < alpha? %s\n" % ( (p_value < alpha), ) )
    
    def saveUncertaintyBoundsComparisonPlot(self, outDir, filename, lowerBound, upperBound,
                                            format='PDF', log=False, xlabel=None, ylabel=None,
                                            title=None, plotObs=True, plotMedian=False, plotColor=False,
                                            legend_items=None, sizeX=1, sizeY=1, dpi=80, opacity=0.5,
                                            ks_stat=False):
        """ Save uncertainty bounds plot to outDir
        
            @param lowerBound Float <100.0, >0.0, <upperBound
            @param upperBound Float <100.0, >0.0, >lowerBound
        """
        assert( format in ['PDF', 'PNG'] )
        if format == 'PDF':
            plotFilename = "%s.pdf" % (filename,)
        elif format == 'PNG':
            plotFilename = "%s.png" % (filename,)
        plotFilepath = os.path.join(outDir, plotFilename)
        
        my_legend_items = None
        if legend_items:
            my_legend_items = []
        
        assert( self.x1 is not None )
        assert( self.ysim1 is not None )
        assert( self.x2 is not None )
        assert( self.ysim2 is not None )
        
        if plotColor:
            fillColor1 = 'black'
            fillColor2 = 'yellow'
            median_color1 = 'black'
            median_color2 = 'yellow'
        else:
            fillColor1 = 'black'
            fillColor2 = '#dddddd'
            median_color1 = 'black'
            median_color2 = '#cccccc'
        
        # Get the uncertainty boundary
        (minYsim1, maxYsim1, medianYsim1, meanYsim1) = \
            calculateUncertaintyBounds(self.ysim1, self.likelihood1,
                                       lowerBound, upperBound)
        (minYsim2, maxYsim2, medianYsim2, meanYsim2) = \
            calculateUncertaintyBounds(self.ysim2, self.likelihood2,
                                       lowerBound, upperBound)
    
        # Are the distributions the same?
        if ks_stat:
            self.calculateKolmogorovSmirnov(medianYsim1, medianYsim2)
    
        # Plot it up
        fig = plt.figure(figsize=(sizeX, sizeY), dpi=dpi, tight_layout=True)
        ax = fig.add_subplot(121)
        
        data_plt = []
        # Draw shaded uncertainty envelope
        ax.fill_between(self.x1, minYsim1, maxYsim1, linewidth=0,
                        color=fillColor1)
        ax.fill_between(self.x2, minYsim2, maxYsim2, linewidth=0,
                        color=fillColor2, alpha=opacity)

        # Draw observed line
        if plotMedian:
            (p, ) = ax.plot(self.x1, medianYsim1, color=median_color1, linestyle='solid')
            data_plt.append(p)
            
            (p, ) = ax.plot(self.x2, medianYsim2, color=median_color2, linestyle='solid')
            data_plt.append(p)
            
            if(legend_items):
                my_legend_items.append("Median %s" % (legend_items[0]) )
                my_legend_items.append("Median %s" % (legend_items[1]) )
        
        # Annotations
        # X-axis
        quarterly = matplotlib.dates.MonthLocator(interval=3)
        ax.xaxis.set_major_locator(quarterly)
        ax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter('%b-%Y') )
        # Rotate
        plt.setp( ax.xaxis.get_majorticklabels(), rotation=45 )
        plt.setp( ax.xaxis.get_majorticklabels(), fontsize=6 )
        
        # Y-axis
        plt.setp( ax.get_yticklabels(), fontsize=6 )
        
        if log:
            ax.set_yscale('log')
        
        if xlabel:
            ax.set_xlabel(xlabel)
        if ylabel:
            ax.set_ylabel(ylabel)
        if title:
            fig.suptitle(title, y=1.03)
    
        # Plot exceedance plot
        ax2 = fig.add_subplot(122)
        
        num_data = max( len(self.ysim1), len(self.ysim2) )
        min_x = min( np.min(self.ysim1), np.min(self.ysim2) )
        max_x = max( np.max(self.ysim1), np.max(self.ysim2) )
        x = np.linspace(min_x, max_x, num=num_data )
        
        if plotMedian:
            med_ecdf1 = sm.distributions.ECDF(medianYsim1)
            med_y1 = 1 - med_ecdf1(x)
            ax2.plot(med_y1, x, color=median_color1, linestyle='solid')
            
            med_ecdf2 = sm.distributions.ECDF(medianYsim2)
            med_y2 = 1 - med_ecdf2(x)
            ax2.plot(med_y2, x, color=median_color2, linestyle='solid')
        
        min_ecdf1 = sm.distributions.ECDF(minYsim1)
        min_y1 = 1 - min_ecdf1(x)
        ax2.plot(min_y1, x, color=fillColor1, linestyle='dashed')
        
        min_ecdf2 = sm.distributions.ECDF(minYsim2)
        min_y2 = 1 - min_ecdf2(x)
        ax2.plot(min_y2, x, color=fillColor2, linestyle='dashed')
        
        max_ecdf1 = sm.distributions.ECDF(maxYsim1)
        max_y1 = 1 - max_ecdf1(x)
        ax2.plot(max_y1, x, color=fillColor1, linestyle='dashed')
        
        max_ecdf2 = sm.distributions.ECDF(maxYsim2)
        max_y2 = 1 - max_ecdf2(x)
        ax2.plot(max_y2, x, color=fillColor2, linestyle='dashed')
        
        # Annotations
        # X-axis
        formatter = FuncFormatter(to_percent)
        ax2.xaxis.set_major_formatter(formatter)
        ax2.set_xlabel('Exceedance probability (%)')
        
        # Y-axis
        if log:
            ax2.set_yscale('log')
        else:
            ax2.set_ylim( ax.get_ylim() )
        
        plt.setp( ax2.get_xticklabels(), fontsize=6 )
        plt.setp( ax2.get_yticklabels(), fontsize=6 )
    
        # Plot legend last
        if legend_items:
            legend = fig.legend( data_plt, my_legend_items, 'lower center', fontsize='x-small', 
                                ncol=len(my_legend_items), frameon=True )
            frame = legend.get_frame()
            frame.set_facecolor('0.25')
            frame.set_alpha(0.5)
            
        # Save the figure
        fig.savefig(plotFilepath, format=format, bbox_inches='tight', pad_inches=0.25)
    
    def main(self, args):
        # Set up command line options
        parser = argparse.ArgumentParser(description="Tool for making comparative visualizions of output from two behavioral model runs for RHESSys")
        parser.add_argument("basedirs", metavar="BASEDIRS", nargs=2,
                            help="Base directories of the two calibration sessions to be compared")
        
        parser.add_argument("sessions", metavar="SESSIONS", type=int, nargs=2,
                            help="Session IDs of the two behavioral runs.")
        
        parser.add_argument("-t", "--title",
                            dest="title",
                            help="Title to add to output figures")
        
        parser.add_argument("-o", "--outdir", required=False, default=os.getcwd(),
                            dest="outdir",
                            help="Output directory in which to place figures.  If not specified, the currect directory will be used.")

        parser.add_argument("-of", "--outputFormat", action="store",
                            dest="outputFormat", default="PDF", choices=["PDF", "PNG"],
                            help="Output format to save figures in.")

        parser.add_argument("--behavioral_filter", action="store",
                            dest="behavioral_filter", required=False,
                            default="nse>0.5 and nse_log>0.5",
                            help="SQL where clause to use to determine which runs qualify as behavioral parameters.  E.g. 'nse>0.5 AND nse_log>0.5' (use quotes)")

        parser.add_argument("--supressObs", action="store_true", required=False, default=False,
                            help="Do not plot observered data")

        parser.add_argument("--plotMedian", action="store_true", required=False, default=False,
                            help="Plot mean value of behavioral runs")

        parser.add_argument("--color", action="store_true", required=False, default=False,
                            help="Plot in color")
        
        parser.add_argument("--opacity", action="store", required=False, 
                            type=float, default=0.5,
                            help="Opacity used to draw shaded uncertainty region of second calibration session.")
        
        parser.add_argument("-l", "--legend", action="store", required=False, nargs=2,
                            dest="legend_items",
                            help="Legend items. If not supplied, no legend will be printed")
        
        parser.add_argument('--figureX', required=False, type=int, default=4,
                            help='The width of the plot, in inches')
    
        parser.add_argument('--figureY', required=False, type=int, default=3,
                            help='The height of the plot, in inches')

        parser.add_argument("--loglevel", action="store",
                            dest="loglevel", default="OFF",
                            help="Set logging level, one of: OFF [default], DEBUG, CRITICAL (case sensitive)")
        
        options = parser.parse_args()
        
        # Base of output filename
        behavioralFilename = 'behavioral_comparison'
        
        # Enforce initial command line options rules
        if "DEBUG" == options.loglevel:
            self._initLogger(logging.DEBUG)
        elif "CRITICAL" == options.loglevel:
            self._initLogger(logging.CRITICAL)
        else:
            self._initLogger(logging.NOTSET)
        
        basedir1 = options.basedirs[0]
        session1 = options.sessions[0]
        if not os.path.isdir(basedir1) or not os.access(basedir1, os.R_OK):
            sys.exit("Unable to access basedir %s" % (basedir1,) )
        behavioralFilename += '_' + os.path.basename(basedir1)
        basedir1 = os.path.abspath(basedir1)
        
        basedir2 = options.basedirs[1]
        session2 = options.sessions[1]
        if not os.path.isdir(basedir2) or not os.access(basedir2, os.R_OK):
            sys.exit("Unable to access basedir %s" % (basedir2,) )
        behavioralFilename += '_V_' + os.path.basename(basedir2)
        basedir2 = os.path.abspath(basedir2)
            
        if not os.path.isdir(options.outdir) and os.access(options.outdir, os.W_OK):
            parser.error("Figure output directory %s must be a writable directory" % (options.outdir,) )
        outdirPath = os.path.abspath(options.outdir)

        try:
            # Read data for first behavioral run
            (runsProcessed1, self.obs1, self.x1, self.ysim1, self.likelihood1) = \
                self.readBehavioralData(basedir1, session1, 'streamflow',
                                        behavioral_filter=options.behavioral_filter)
            
            # Read data for second behavioral run
            (runsProcessed2, self.obs2, self.x2, self.ysim2, self.likelihood2) = \
                self.readBehavioralData(basedir2, session2, 'streamflow',
                                        behavioral_filter=options.behavioral_filter)

            if runsProcessed1 and runsProcessed2:
                if options.supressObs:
                    behavioralFilename += '_noObs'
                if options.plotMedian:
                    behavioralFilename += '_median'
                if options.color:
                    behavioralFilename += '_color'
                if options.legend_items:
                    behavioralFilename += '_legend'
                # Generate visualizations
                self.saveUncertaintyBoundsComparisonPlot(outdirPath, behavioralFilename, 
                                               2.5, 97.5, format=options.outputFormat, log=False,
                                               ylabel=r'Streamflow ($mm^{-d}$)',
                                               title=options.title, 
                                               plotObs=(not options.supressObs),
                                               plotMedian=options.plotMedian,
                                               plotColor=options.color,
                                               legend_items=options.legend_items,
                                               sizeX=options.figureX, sizeY=options.figureY,
                                               opacity=options.opacity )
                behavioralFilename += '-log'
                self.saveUncertaintyBoundsComparisonPlot(outdirPath, behavioralFilename, 
                                               2.5, 97.5, format=options.outputFormat, log=True,
                                               ylabel=r'Streamflow ($mm^{-d}$)',
                                               title=options.title,
                                               plotObs=(not options.supressObs),
                                               plotMedian=options.plotMedian,
                                               plotColor=options.color,
                                               legend_items=options.legend_items,
                                               sizeX=options.figureX, sizeY=options.figureY,
                                               opacity=options.opacity, ks_stat=True )
            else:
                if not runsProcessed1:
                    errorStr = "Did not read any behavioral data for basedir %s, session %d"
                    self.logger.error(errorStr % \
                                      (basedir1, session1) )
                if not runsProcessed2:
                    self.logger.error(errorStr % \
                                      (basedir2, session2) )
                return 1
        except:
            raise
        else:
            self.logger.debug("exiting normally")
            return 0
        finally:
            # Decrement reference count, this will (hopefully) allow __del__
            #  to be called on the once referenced object
            calibratorDB = None

class BehavioralTimeseriesOut(RHESSysCalibratorPostprocessBehavioral):
    
    TIMESERIES_EXT = os.extsep + 'csv'
    
    def main(self, args):
        # Set up command line options
        parser = argparse.ArgumentParser(description="Output timeseries from behavioral model runs of RHESSys")
        parser.add_argument("-b", "--basedir", action="store", 
                            dest="basedir", required=True,
                            help="Base directory for the calibration session")
        
        parser.add_argument("-s", "--behavioral_session", action="store", type=int,
                            dest="session_id", required=True,
                            help="Session to use for behavioral runs.")
        
        parser.add_argument("-o", "--outdir", action="store", required=False,
                            dest="outdir",
                            help="Output directory in which to place figures.  If not specified, basedir will be used.")

        parser.add_argument("-f", "--file", action="store", required=False, default='behavioral_ts',
                            dest="outfile",
                            help="The base name of the output timeseries. Files will be created for min, max, mean, and median timesers. Filenames will end in '.csv'")

        parser.add_argument("--lowerBound", required=False, type=float, default=2.5,
                            help="Percentile of lower bound of uncertainty range")

        parser.add_argument("--upperBound", required=False, type=float, default=97.5,
                            help="Percentile of upper bound of uncertainty range")

        parser.add_argument("--behavioral_filter", action="store",
                            dest="behavioral_filter", required=False,
                            default="nse>0.5 and nse_log>0.5",
                            help="SQL where clause to use to determine which runs qualify as behavioral parameters.  E.g. 'nse>0.5 AND nse_log>0.5' (use quotes)")

        parser.add_argument("-l", "--loglevel", action="store",
                            dest="loglevel", default="OFF",
                            help="Set logging level, one of: OFF [default], DEBUG, CRITICAL (case sensitive)")
        
        options = parser.parse_args()
        
        # Enforce initial command line options rules
        if "DEBUG" == options.loglevel:
            self._initLogger(logging.DEBUG)
        elif "CRITICAL" == options.loglevel:
            self._initLogger(logging.CRITICAL)
        else:
            self._initLogger(logging.NOTSET)
        
        if not os.path.isdir(options.basedir) or not os.access(options.basedir, os.W_OK):
            sys.exit("Unable to write to basedir %s" % (options.basedir,) )
        basedir = os.path.abspath(options.basedir)
        
        if not options.outdir:
            options.outdir = basedir
            
        if not os.path.isdir(options.outdir) and os.access(options.outdir, os.W_OK):
            parser.error("Figure output directory %s must be a writable directory" % (options.outdir,) )
        outdirPath = os.path.abspath(options.outdir)

        try:
            (runsProcessed, self.obs, self.x, self.ysim, self.likelihood) = \
                self.readBehavioralData(basedir, options.session_id, 'streamflow',
                                        observed_file=None, behavioral_filter=options.behavioral_filter)
            if runsProcessed:
                # Get the uncertainty boundary
                colNames = ['streamflow']
                indexLabel = 'datetime'
                
                (minYsim, maxYsim, medianYsim, meanYsim) = \
                    calculateUncertaintyBounds(self.ysim, self.likelihood,
                                               options.lowerBound, options.upperBound)
                
                behavioralFilename = "%s_SESSION_%s" % ( options.outfile, options.session_id )
                behavioralFilepath = os.path.join( outdirPath, behavioralFilename )
                # Write out min timeseries
                minFilepath = behavioralFilepath + '_min' + BehavioralTimeseriesOut.TIMESERIES_EXT
                min_ts = pd.Series(minYsim, index=self.x, name=colNames)
                min_ts.to_csv(minFilepath, header=True, index_label=indexLabel)
                
                # Write out max timeseries
                maxFilepath = behavioralFilepath + '_max' + BehavioralTimeseriesOut.TIMESERIES_EXT
                max_ts = pd.Series(maxYsim, index=self.x, name=colNames)
                max_ts.to_csv(maxFilepath, header=True, index_label=indexLabel)
                
                # Write out median timeseries
                medianFilepath = behavioralFilepath + '_median' + BehavioralTimeseriesOut.TIMESERIES_EXT
                median_ts = pd.Series(medianYsim, index=self.x, name=colNames)
                median_ts.to_csv(medianFilepath, header=True, index_label=indexLabel)
                
                # Write out mean timeseries
                meanFilepath = behavioralFilepath + '_mean' + BehavioralTimeseriesOut.TIMESERIES_EXT
                mean_ts = pd.Series(meanYsim, index=self.x, name=colNames)
                mean_ts.to_csv(meanFilepath, header=True, index_label=indexLabel)
                
        except:
            raise
        else:
            self.logger.debug("exiting normally")
            return 0
        finally:
            # Decrement reference count, this will (hopefully) allow __del__
            #  to be called on the once referenced object
            calibratorDB = None
    