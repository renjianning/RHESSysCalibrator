"""@package rhessyscalibrator.postprocess_behavioral

@brief Tool for analyzing model run results generated by rhessys_calibrator_behavioral.py and 
stored in a database format managed by rhessyscalibrator.model_runner_db.py.  Currently
plots 95% uncertainty bounds around observed streamflow using NSE as a likelihood function.

This software is provided free of charge under the New BSD License. Please see
the following license information:

Copyright (c) 2013, University of North Carolina at Chapel Hill
All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above copyright
      notice, this list of conditions and the following disclaimer in the
      documentation and/or other materials provided with the distribution.
    * Neither the name of the University of North Carolina at Chapel Hill nor the
      names of its contributors may be used to endorse or promote products
      derived from this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE UNIVERSITY OF NORTH CAROLINA AT CHAPEL HILL
BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR 
CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE
GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT 
LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT
OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.


@author Brian Miles <brian_miles@unc.edu>
"""
import os, sys, errno
import argparse
import logging
from datetime import datetime

import math
import numpy as np
import pandas as pd
from scipy import stats
import statsmodels.api as sm
import matplotlib
matplotlib.use('Agg') # Allow for running on machines without X servers
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter

from rhessysworkflows.rhessys import RHESSysOutput

from rhessyscalibrator.calibrator import RHESSysCalibrator
from rhessyscalibrator import postprocess
from rhessyscalibrator.model_runner_db2 import *


def exceedance_prob(y):
    num_y = len(y)
    sorted = np.sort(y)
    sorted = sorted[::-1] # Reverse order
    rank = np.arange(1, num_y+1)
    prob = rank.astype(float) / float(num_y)
    prob[0] = 0
    
    return (prob, sorted)


def to_percent(y, position):
    # Ignore the passed in position. This has the effect of scaling the default
    # tick locations.
    s = "%.0f" % ( (100 * y), )

    # The percent symbol needs escaping in latex
    if matplotlib.rcParams['text.usetex'] == True:
        return s + r'$\%$'
    else:
        return s + '%'


def calculateUncertaintyBounds(ysim, likelihood, lowerBound, upperBound):
    """ Calculate uncertainty bounds for matrix of simulated values
    
        @param ysim Numpy array containing a vector of data for a number of simuations,
          dimensions [NUM_SIMULATIONS, NUM_DATA_PER_SIMULATION]
        @param likelihood Numpy array containing model fitness parameter for each
          simulation, dimensions [NUM_SIMULATIONS]
        @param lowerBound Double representing percentile of lower bound of confidence interval (e.g. 2.5)
        @param upperBound Double representing percentile of upper bound of confidence interval (e.g. 97.5)
        
        @return Tuple of numpy arrays representing minYsim, maxYsim, and medianYsim, dimensions
          [NUM_DATA_PER_SIMULATION]  
    """
    assert( lowerBound < 100.0 and lowerBound > 0.0)
    assert( upperBound < 100.0 and upperBound > 0.0)
    assert( lowerBound < upperBound )
    
    # Normalize likelihood to have values from 0 to 1
    normLH = likelihood / np.sum(likelihood)
    
    lower = lowerBound / 100.0
    upper = upperBound / 100.0

    nIters = np.shape(ysim)[1]
    minYsim = np.zeros(nIters)
    maxYsim = np.zeros(nIters)
    medianYsim = np.zeros(nIters)
    meanYsim = np.zeros(nIters)

    # Generate uncertainty interval bounded by lower bound and upper bound
    for i in xrange(0, nIters):
        ys = ysim[:,i]
        # Use CDF of likelihood values as basis for interval
        sortedIdx = np.argsort(ys)
        sortYsim = ys[sortedIdx]
        sortLH = normLH[sortedIdx]
        cumLH = np.cumsum(sortLH)
        cond = (cumLH > lower) & (cumLH < upper)
            
        validYsim = sortYsim[cond]
        minYsim[i] = validYsim[0]
        maxYsim[i] = validYsim[-1]
        medianYsim[i] = np.median(validYsim)
        meanYsim[i] = np.mean(validYsim)
        
    return (minYsim, maxYsim, medianYsim, meanYsim)


def calculateWeightedEnsembleMean(ysim, likelihood):
    """ Calculate weighted ensemble mean (Seibert and Beven 2009).  For each time step,
        weights are 0.02 for the "best" model run, 0.0 for the "worst" (based on likelihood) 
        for each. 
    
        @param ysim Numpy array containing a vector of data for a number of simuations,
          dimensions [NUM_SIMULATIONS, NUM_DATA_PER_SIMULATION]
        @param likelihood Numpy array containing model fitness parameter for each
          simulation, dimensions [NUM_SIMULATIONS]
          
        @return Numpy array representing weighted ensemble mean.
    """
    (nSim, nIters) = np.shape(ysim)
    weightedEnsembleMean = np.zeros(nIters)
    
    max_weight = (1.0 / nSim) * 2
    weights = np.linspace(max_weight, 0.0, num=nSim)
    
    for i in xrange(0, nIters):
        ys = ysim[:,i]
        sortedIdx = np.argsort(likelihood)
        sortYsim = ys[sortedIdx]
        sortLH = likelihood[sortedIdx]
        
        weightedEnsembleMean[i] = np.sum( weights * sortYsim )
        
    return weightedEnsembleMean
        
        
class RHESSysCalibratorPostprocessBehavioral(object):
    """ Main driver class for rhessys_calibrator_postprocess_behavioral tool
    """
    
    def saveUncertaintyBoundsPlot(self, outDir, filename, lowerBound, upperBound,
                                  format='PDF', log=False, xlabel=None, ylabel=None,
                                  title=None, plotObs=True, plotMedian=False, plotMean=False, 
                                  plotWeightedMean=False,
                                  plotColor=False, plotExceedance=True, 
                                  obsStyle='solid', legend=True, sizeX=1, sizeY=1, dpi=80):
        """ Save uncertainty bounds plot to outDir
        
            @param lowerBound Float <100.0, >0.0, <upperBound
            @param upperBound Float <100.0, >0.0, >lowerBound
        """
        assert( format in ['PDF', 'PNG'] )
        if format == 'PDF':
            plotFilename = "%s.pdf" % (filename,)
        elif format == 'PNG':
            plotFilename = "%s.png" % (filename,)
        plotFilepath = os.path.join(outDir, plotFilename)
        
        assert( self.x is not None )
        assert( self.ysim is not None )
        
        if plotColor:
            fillColor = '0.5'
            obs_color = 'blue'
            median_color = 'magenta'
            mean_color = 'green'
            weighted_mean_color = 'orange'
            min_color = 'grey'
            max_color = 'grey'
        else:
            fillColor = '0.5'
            obs_color = 'black'
            median_color = '0.75'
            mean_color = '0.25'
            weighted_mean_color = '0.667'
            min_color = 'grey'
            max_color = 'grey'
        
        wghtMean = None
        if plotWeightedMean:
            wghtMean = calculateWeightedEnsembleMean(self.ysim, self.likelihood)
        
        # Get the uncertainty boundary
        (minYsim, maxYsim, medianYsim, meanYsim) = \
            calculateUncertaintyBounds(self.ysim, self.likelihood,
                                       lowerBound, upperBound)
            
        # Calculate percent observations within uncertainty bounds
        numObs = len(self.obs)
        print("\nNumber of observations: %d" % (numObs,) )
        obsLteMax = np.where(self.obs <= maxYsim)[0]
        obsGteMin = np.where(self.obs >= minYsim)[0]
        obsInBounds = np.intersect1d(obsLteMax, obsGteMin)
        numObsInBounds = len(obsInBounds)
        pctObsInBounds = ( float( numObsInBounds) / float( numObs ) ) * 100.0
        print("Observations within prediction bounds: %d (%.2f%%)\n" % \
              (numObsInBounds, pctObsInBounds) )
        
        # Calculate Average Relative Interval Length (ARIL) as per Dotto et al. 2012
        ARIL = (1 / float(numObs) ) * np.sum( (maxYsim - minYsim) / self.obs )
        print("ARIL: %.2f\n" % (ARIL,) )
        
        # Plot it up
        fig = plt.figure(figsize=(sizeX, sizeY), dpi=dpi, tight_layout=True)
        if plotExceedance:
            ax = fig.add_subplot(121)
        else:
            ax = fig.add_subplot(111)
        
        data_plt = []
        legend_items = []
        # Draw observed line
        if plotObs:
            (p, ) = ax.plot(self.x, self.obs, color=obs_color, linestyle=obsStyle)
            data_plt.append(p)
            legend_items.append('Observed data')
        if plotMedian:
            (p, ) = ax.plot(self.x, medianYsim, color=median_color, linestyle='solid')
            data_plt.append(p)
            legend_items.append('Median simulated')
        if plotMean:
            (p, ) = ax.plot(self.x, meanYsim, color=mean_color, linestyle='solid')
            data_plt.append(p)
            legend_items.append('Mean simulated')
        if plotWeightedMean:
            (p, ) = ax.plot(self.x, wghtMean, color=weighted_mean_color, linestyle='solid')
            data_plt.append(p)
            legend_items.append('Weighted ensemble mean simulated')
        # Draw shaded uncertainty envelope
        ax.fill_between(self.x, minYsim, maxYsim, color=fillColor)
        
        # Annotations
        # X-axis
        quarterly = matplotlib.dates.MonthLocator(interval=3)
        ax.xaxis.set_major_locator(quarterly)
        ax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter('%b-%Y') )
        # Rotate
        plt.setp( ax.xaxis.get_majorticklabels(), rotation=45 )
        plt.setp( ax.xaxis.get_majorticklabels(), fontsize=6 )
        
        # Y-axis
        plt.setp( ax.get_yticklabels(), fontsize=6 )
        
        if log:
            ax.set_yscale('log')
        
        if xlabel:
            ax.set_xlabel(xlabel)
        if ylabel:
            ax.set_ylabel(ylabel)
        if title:
            fig.suptitle(title, y=1.03)
        
        # Plot exceedance plot
        if plotExceedance:
            ax2 = fig.add_subplot(122)
            
            obs_plt = None
            if plotObs:
                x, y = exceedance_prob(self.obs)
                ax2.plot(x, y, color=obs_color)
            if plotMedian:
                x, y = exceedance_prob(medianYsim)
                ax2.plot(x, y, color=median_color, linestyle='solid')
            if plotMean:
                x, y = exceedance_prob(meanYsim)
                ax2.plot(x, y, color=mean_color, linestyle='solid')
            if plotWeightedMean:
                x, y = exceedance_prob(wghtMean)
                ax2.plot(x, y, color=weighted_mean_color, linestyle='solid')
            
            x, y = exceedance_prob(minYsim)
            ax2.plot(x, y, color=min_color, linestyle='dashed')
    
            x, y = exceedance_prob(maxYsim)
            ax2.plot(x, y, color=max_color, linestyle='dashed')
            
            # Annotations
            # X-axis
            formatter = FuncFormatter(to_percent)
            ax2.xaxis.set_major_formatter(formatter)
            ax2.set_xlabel('Exceedance probability (%)')
            
            # Y-axis
            if log:
                ax2.set_yscale('log')
            else:
                ax2.set_ylim( ax.get_ylim() )
            
            plt.setp( ax2.get_xticklabels(), fontsize=6 )
            plt.setp( ax2.get_yticklabels(), fontsize=6 )
        
        # Plot legend last
        if legend:
            legend = fig.legend( data_plt, legend_items, 'lower center', fontsize='x-small', 
                                ncol=len(legend_items), frameon=True )
            frame = legend.get_frame()
            frame.set_facecolor('0.9')
            frame.set_alpha(0.5)
        
        # Save the figure
        fig.savefig(plotFilepath, format=format, bbox_inches='tight', pad_inches=0.25)
    
    def _initLogger(self, level):
        """ Setup logger.  Log to the console for now """
        self.logger = logging.getLogger("cluster_calibrator")
        self.logger.setLevel(level)
        # console handler and set it to level
        consoleHandler = logging.StreamHandler()
        consoleHandler.setLevel(level)
        # create formatter
        formatter = \
            logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
        # add formatter to console handler
        consoleHandler.setFormatter(formatter)
        # add consoleHandler to logger
        self.logger.addHandler(consoleHandler)
        
        
    def readBehavioralData(self, basedir, postprocess_id, variable='streamflow',
                           observed_file=None, behavioral_filter=None, end_date=None):
        
        dbPath = RHESSysCalibrator.getDBPath(basedir)
        if not os.access(dbPath, os.R_OK):
            raise IOError(errno.EACCES, "The database at %s is not readable" %
                          dbPath)
        self.logger.debug("DB path: %s" % dbPath)
        
        outputPath = RHESSysCalibrator.getOutputPath(basedir)
        if not os.access(outputPath, os.R_OK):
            raise IOError(errno.EACCES, "The output directory %s is  not readable" % outputPath)
        self.logger.debug("Output path: %s" % outputPath)

        rhessysPath = RHESSysCalibrator.getRhessysPath(basedir)
        
        calibratorDB = \
            ModelRunnerDB2(RHESSysCalibrator.getDBPath(
                basedir))
        
        # Get post-process session
        postproc = calibratorDB.getPostProcess(postprocess_id)
        if None == postproc:
            raise Exception("Post-process session %d was not found in the calibration database %s" % (postprocess_id, dbPath))
        # Get session
        session = calibratorDB.getSession(postproc.session_id)
        if None == session:
            raise Exception("Session %d was not found in the calibration database %s" % (postproc.session_id, dbPath))
        if session.status != "complete":
            print "WARNING: session status is: %s.  Some model runs may not have completed." % (session.status,)
        else:
            self.logger.debug("Session status is: %s" % (session.status,))
        
        # Determine observation file path
        if observed_file:
            obs_file = observed_file
        else:
            # Get observered file from post-process session
            assert( postproc.obs_filename != None )
            obs_file = postproc.obs_filename
        obsPath = RHESSysCalibrator.getObsPath(basedir)
        obsFilePath = os.path.join(obsPath, obs_file)
        if not os.access(obsFilePath, os.R_OK):
            raise IOError(errno.EACCES, "The observed data file %s is  not readable" % obsFilePath)
        self.logger.debug("Obs path: %s" % obsFilePath)
        
        # Get runs in session
        runs = calibratorDB.getRunsInPostProcess(postprocess_id, where_clause=behavioral_filter)
        numRuns = len(runs) 
        if numRuns == 0:
            raise Exception("No runs found for post process session %d, calibration session %d" 
                            % (postprocess_id, session.id))
        response = raw_input("%d runs selected for plotting from post process session %d, calibration session %d in basedir '%s', continue? [yes | no] " % \
                            (numRuns, postprocess_id, session.id, os.path.basename(basedir) ) )
        response = response.lower()
        if response != 'y' and response != 'yes':
            # Exit normally
            return 0
        self.logger.debug("%d behavioral runs" % (numRuns,) )
        
        # Read observed data from file
        obs_all = pd.read_csv(obsFilePath, index_col=0, parse_dates=True)
        obs = obs_all[postprocess.OBS_HEADER_STREAMFLOW]
        if end_date:
            obs = obs[:end_date]
        
        likelihood = np.empty(numRuns)
        ysim = None
        x = None
        
        runsProcessed = False
        for (i, run) in enumerate(runs):
            if "DONE" == run.status:
                runOutput = os.path.join(rhessysPath, run.output_path)
                self.logger.debug(">>>\nOutput dir of run %d is %s" %
                                     (run.id, runOutput))
                tmpOutfile = \
                    RHESSysCalibrator.getRunOutputFilePath(runOutput)
                if not os.access(tmpOutfile, os.R_OK):
                    print "Output file %s for run %d not found or not readable, unable to calculate fitness statistics for this run" % (tmpOutfile, run.id)
                    continue
                
                tmpFile = open(tmpOutfile, 'r')
                 
                (tmp_datetime, tmp_data) = \
                        RHESSysOutput.readColumnFromFile(tmpFile,
                                                         "streamflow", startHour=0)
                tmp_mod = pd.Series(tmp_data, index=tmp_datetime)
                # Align timeseries to observed
                (mod, obs) = tmp_mod.align(obs, join='inner')
                                 
                # Stash date for X values (assume they are the same for all runs
                if x == None:
                    x = [datetime.strptime(str(d), '%Y-%m-%d %H:%M:%S') for d in mod.index]
        
                # Put data in matrix
                dataLen = len(mod)
                if ysim is None:
                    # Allocate matrix for results
                    ysim = np.empty( (numRuns, dataLen) )
                assert( np.shape(ysim)[1] == dataLen )
                ysim[i,] = mod
                
                # Store fitness parameter
                likelihood[i] = run.run_fitness.nse
                        
                tmpFile.close()                
                runsProcessed = True
        
        return (runsProcessed, obs, x, ysim, likelihood)
    
    
    def readBehavioralDataMulti(self, basedir, postprocess_id, cols=['streamflow'],
                                observed_file=None, behavioral_filter=None, end_date=None):
        
        dbPath = RHESSysCalibrator.getDBPath(basedir)
        if not os.access(dbPath, os.R_OK):
            raise IOError(errno.EACCES, "The database at %s is not readable" %
                          dbPath)
        self.logger.debug("DB path: %s" % dbPath)
        
        outputPath = RHESSysCalibrator.getOutputPath(basedir)
        if not os.access(outputPath, os.R_OK):
            raise IOError(errno.EACCES, "The output directory %s is  not readable" % outputPath)
        self.logger.debug("Output path: %s" % outputPath)

        rhessysPath = RHESSysCalibrator.getRhessysPath(basedir)
        
        calibratorDB = \
            ModelRunnerDB2(RHESSysCalibrator.getDBPath(
                basedir))
        
        # Get post-process session
        postproc = calibratorDB.getPostProcess(postprocess_id)
        if None == postproc:
            raise Exception("Post-process session %d was not found in the calibration database %s" % (postprocess_id, dbPath))
        # Get session
        session = calibratorDB.getSession(postproc.session_id)
        if None == session:
            raise Exception("Session %d was not found in the calibration database %s" % (postproc.session_id, dbPath))
        if session.status != "complete":
            print "WARNING: session status is: %s.  Some model runs may not have completed." % (session.status,)
        else:
            self.logger.debug("Session status is: %s" % (session.status,))
        
        # Determine observation file path
        if observed_file:
            obs_file = observed_file
        else:
            # Get observered file from session
            assert( session.obs_filename != None )
            obs_file = session.obs_filename
        obsPath = RHESSysCalibrator.getObsPath(basedir)
        obsFilePath = os.path.join(obsPath, obs_file)
        if not os.access(obsFilePath, os.R_OK):
            raise IOError(errno.EACCES, "The observed data file %s is  not readable" % obsFilePath)
        self.logger.debug("Obs path: %s" % obsFilePath)
        
        # Get runs in session
        runs = calibratorDB.getRunsInPostProcess(postprocess_id, where_clause=behavioral_filter)
        numRuns = len(runs) 
        if numRuns == 0:
            raise Exception("No runs found for post process session %d, calibration session %d" 
                            % (postprocess_id, session.id))
        response = raw_input("%d runs selected for plotting from post process session %d, calibration session %d in basedir '%s', continue? [yes | no] " % \
                            (numRuns, postprocess_id, session.id, os.path.basename(basedir) ) )
        response = response.lower()
        if response != 'y' and response != 'yes':
            # Exit normally
            return 0
        self.logger.debug("%d behavioral runs" % (numRuns,) )
        
        # Read observed data from file
        obs_all = pd.read_csv(obsFilePath, index_col=0, parse_dates=True)
        obs = obs_all[postprocess.OBS_HEADER_STREAMFLOW]
        if end_date:
            obs = obs[:end_date]
        
        numCols = len(cols)
        likelihood = np.empty(numRuns)
        sim = None
        x = None
        
        runsProcessed = False
        obs_day = obs.resample('D', how='sum') # Get rid of useless hour from index
        for (i, run) in enumerate(runs):
            if "DONE" == run.status:
                runOutput = os.path.join(rhessysPath, run.output_path)
                self.logger.debug(">>>\nOutput dir of run %d is %s" %
                                     (run.id, runOutput))
                tmpOutfile = \
                    RHESSysCalibrator.getRunOutputFilePath(runOutput)
                if not os.access(tmpOutfile, os.R_OK):
                    print "Output file %s for run %d not found or not readable, unable to calculate fitness statistics for this run" % (tmpOutfile, run.id)
                    continue
                
                tmpFile = open(tmpOutfile, 'r')
                tmp_data = RHESSysOutput.readColumnsFromFile(tmpFile, cols)
                # Align timeseries to observed
                (mod_align, obs_align) = tmp_data.align(obs_day, axis=0, join='inner')
                                 
                # Stash date for X values (assume they are the same for all runs
                if x == None:
                    x = [datetime.strptime(str(d), '%Y-%m-%d %H:%M:%S') for d in mod_align.index]
                # Put data in matrix
                dataLen = len(mod_align)
                if sim == None:
                    # Allocate list for results
                    sim = []
                sim.append(mod_align)
                
                # Store fitness parameter
                likelihood[i] = run.run_fitness.nse
                        
                tmpFile.close()                
                runsProcessed = True
        
        return (runsProcessed, obs_align, x, sim, likelihood)
    
    
    def main(self, args):
        # Set up command line options
        parser = argparse.ArgumentParser(description="Tool for visualizing output from behavioral model runs for RHESSys")
        parser.add_argument("-b", "--basedir", action="store", 
                            dest="basedir", required=True,
                            help="Base directory for the calibration session")
        
        parser.add_argument("-s", "--postprocess_session", action="store", type=int,
                            dest="postprocess_id", required=True,
                            help="Post-process session to use for behavioral runs.")
        
        parser.add_argument("-t", "--title", action="store",
                            dest="title",
                            help="Title to add to output figures")
        
        parser.add_argument("-o", "--outdir", action="store",
                            dest="outdir",
                            help="Output directory in which to place figures.  If not specified, basedir will be used.")

        parser.add_argument("-f", "--file", action="store",
                            dest="observed_file",
                            help="The name of the observed file to use for calculating model fitness statistics.  Filename will be interpreted as being relative to $BASEDIR/obs. If not supplied observation file from calibration model run will be used.")

        parser.add_argument("--enddate", type=int, nargs=4,
                            help="Date on which to end fitness calculationss, of format YYYY M D H")

        parser.add_argument("-of", "--outputFormat", action="store",
                            dest="outputFormat", default="PDF", choices=["PDF", "PNG"],
                            help="Output format to save figures in.")

        parser.add_argument("--behavioral_filter", action="store",
                            dest="behavioral_filter", required=False,
                            default="nse>0.5 and nse_log>0.5",
                            help="SQL where clause to use to determine which runs qualify as behavioral parameters.  E.g. 'nse>0.5 AND nse_log>0.5' (use quotes)")

        parser.add_argument("--supressObs", action="store_true", required=False, default=False,
                            help="Do not plot observered data")
        
        parser.add_argument('--supressExceedance', action="store_true", required=False, default=False,
                            help='Do not generate exceedance plot')

        parser.add_argument("--plotMedian", action="store_true", required=False, default=False,
                            help="Plot median value of behavioral runs")
        
        parser.add_argument("--plotMean", action="store_true", required=False, default=False,
                            help="Plot mean value of behavioral runs")
        
        parser.add_argument("--plotWeightedMean", action="store_true", required=False, default=False,
                            help="Plot weighted ensemble mean value (Seibert and Beven 2009) of behavioral runs")

        parser.add_argument("--obsStyle", required=False, default='solid', choices=['solid', 'dashed', 'dotted'],
                            help="Line style to use for plotting observed timeseries")

        parser.add_argument("--color", action="store_true", required=False, default=False,
                            help="Plot in color")
        
        parser.add_argument("--legend", action="store_true", required=False, default=False,
                            help="Show legend")
        
        parser.add_argument('--figureX', required=False, type=int, default=4,
                            help='The width of the plot, in inches')
    
        parser.add_argument('--figureY', required=False, type=int, default=3,
                            help='The height of the plot, in inches')

        parser.add_argument("-l", "--loglevel", action="store",
                            dest="loglevel", default="OFF",
                            help="Set logging level, one of: OFF [default], DEBUG, CRITICAL (case sensitive)")
        
        options = parser.parse_args()
        
        # Enforce initial command line options rules
        if "DEBUG" == options.loglevel:
            self._initLogger(logging.DEBUG)
        elif "CRITICAL" == options.loglevel:
            self._initLogger(logging.CRITICAL)
        else:
            self._initLogger(logging.NOTSET)
        
        if not os.path.isdir(options.basedir) or not os.access(options.basedir, os.W_OK):
            sys.exit("Unable to write to basedir %s" % (options.basedir,) )
        basedir = os.path.abspath(options.basedir)
        
        if not options.outdir:
            options.outdir = basedir
            
        if not os.path.isdir(options.outdir) and os.access(options.outdir, os.W_OK):
            parser.error("Figure output directory %s must be a writable directory" % (options.outdir,) )
        outdirPath = os.path.abspath(options.outdir)

        endDate = None
        if options.enddate:
            # Set end date based on command line
            endDate = datetime(options.enddate[0],
                               options.enddate[1],
                               options.enddate[2],
                               options.enddate[3])

        plotExceedance = True
        if options.supressExceedance:
            plotExceedance = False

        try:
            print("Using behavioral filter: {0}".format(options.behavioral_filter))
            (runsProcessed, self.obs, self.x, self.ysim, self.likelihood) = \
                self.readBehavioralData(basedir, options.postprocess_id, 'streamflow',
                                        options.observed_file, options.behavioral_filter, end_date=endDate)
            
            if runsProcessed:
                behavioralFilename = 'behavioral'
                if options.supressObs:
                    behavioralFilename += '_noObs'
                if options.supressExceedance:
                    behavioralFilename += '_noExc'
                if options.plotMedian:
                    behavioralFilename += '_median'
                if options.plotMean:
                    behavioralFilename += '_mean'
                if options.plotWeightedMean:
                    behavioralFilename += '_wght_mean'
                if options.color:
                    behavioralFilename += '_color'
                if options.legend:
                    behavioralFilename += '_legend'
                behavioralFilename += "_POSTPROCESS_SESSION_%s" % ( options.postprocess_id, )
                # Generate visualizations
                self.saveUncertaintyBoundsPlot(outdirPath, behavioralFilename, 
                                               2.5, 97.5, format=options.outputFormat, log=False,
                                               ylabel=r'Streamflow ($mm\ day^{-1}$)',
                                               title=options.title, 
                                               plotObs=(not options.supressObs),
                                               plotMedian=options.plotMedian,
                                               plotMean=options.plotMean,
                                               plotWeightedMean=options.plotWeightedMean,
                                               plotColor=options.color, plotExceedance=plotExceedance,
                                               obsStyle=options.obsStyle,
                                               legend=options.legend,
                                               sizeX=options.figureX, sizeY=options.figureY )
                behavioralFilename += '-log'
                self.saveUncertaintyBoundsPlot(outdirPath, behavioralFilename, 
                                               2.5, 97.5, format=options.outputFormat, log=True,
                                               ylabel=r'Streamflow ($mm\ day^{-1}$)',
                                               title=options.title,
                                               plotObs=(not options.supressObs),
                                               plotMedian=options.plotMedian,
                                               plotMean=options.plotMean,
                                               plotWeightedMean=options.plotWeightedMean,
                                               plotColor=options.color, plotExceedance=plotExceedance,
                                               obsStyle=options.obsStyle,
                                               legend=options.legend,
                                               sizeX=options.figureX, sizeY=options.figureY )
        except:
            raise
        else:
            self.logger.debug("exiting normally")
            return 0
        finally:
            # Decrement reference count, this will (hopefully) allow __del__
            #  to be called on the once referenced object
            calibratorDB = None
            
class BehavioralComparison(RHESSysCalibratorPostprocessBehavioral):
    
    def calculateKolmogorovSmirnov(self, data1, data2):
        """ Test whether the behavioral data are from the same distribution using 
            Kolmogorov-Smirnov statistic
            More information: http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ks_2samp.html
                              http://en.wikipedia.org/wiki/Kolmogorov-Smirnov_test
                              http://www.soest.hawaii.edu/wessel/courses/gg313/Critical_KS.pdf
            @param data1 Array of sample data assumed to be drawn from a continuous distribution
            @param data2 Array of sample data assumed to be drawn from a continuous distribution
        """
        # Calculate critical value of K-S statistic D
        ks_crit_table = { 0.10: 1.22, 0.05: 1.36, 0.025: 1.48, 0.01: 1.63, 0.005: 1.73, 0.001: 1.95 }
        #import pdb; pdb.set_trace()
        alpha = 0.05
        c_alpha = ks_crit_table[alpha]
        n1 = len(data1)
        n2 = len(data2)
        ks_crit = c_alpha * math.sqrt( float(n1 + n2) / float(n1 * n2) )
        print("\nCritical value for Kolmogorov-Smirnov statistic (D_alpha; alpha=%.4f): %.4f" % (alpha, ks_crit) )
        
        # Calculate K-S statistic D
        (ks_statistic, p_value) = stats.ks_2samp(data1, data2)
        print("Kolmogorov-Smirnov statistic (D): %.4f" % (ks_statistic,) )
        print("D > D_alpha? %s" % ( (ks_statistic > ks_crit), ) )
        print("\np-value: %.4f" % (p_value,) )
        print("p-value < alpha? %s\n" % ( (p_value < alpha), ) )
    
    
    def saveUncertaintyBoundsComparisonPlot(self, outDir, filename, lowerBound, upperBound,
                                            format='PDF', log=False, xlabel=None, ylabel=None,
                                            title=None, plotObs=True, plotMedian=False, plotWeightedMean=False,
                                            plotColor=False,
                                            legend_items=None, sizeX=1, sizeY=1, dpi=80, opacity=0.5,
                                            ks_stat=False):
        """ Save uncertainty bounds plot to outDir
        
            @param lowerBound Float <100.0, >0.0, <upperBound
            @param upperBound Float <100.0, >0.0, >lowerBound
        """
        assert( format in ['PDF', 'PNG'] )
        if format == 'PDF':
            plotFilename = "%s.pdf" % (filename,)
        elif format == 'PNG':
            plotFilename = "%s.png" % (filename,)
        plotFilepath = os.path.join(outDir, plotFilename)
        
        my_legend_items = None
        if legend_items:
            my_legend_items = []
        
        assert( self.x1 is not None )
        assert( self.ysim1 is not None )
        assert( self.x2 is not None )
        assert( self.ysim2 is not None )
        
        if plotColor:
            fillColor1 = 'black'
            fillColor2 = 'orange'
            median_color1 = 'black'
            median_color2 = 'orange'
            weighted_mean_color1 = 'black'
            weighted_mean_color2 = 'orange'
        else:
            fillColor1 = 'black'
            fillColor2 = '#dddddd'
            median_color1 = 'black'
            median_color2 = '#cccccc'
            weighted_mean_color1 = 'black'
            weighted_mean_color2 = '#cccccc'
        
        wghtMean1 = calculateWeightedEnsembleMean(self.ysim1, self.likelihood1)
        wghtMean2 = calculateWeightedEnsembleMean(self.ysim2, self.likelihood2)
        
        # Get the uncertainty boundary
        (minYsim1, maxYsim1, medianYsim1, meanYsim1) = \
            calculateUncertaintyBounds(self.ysim1, self.likelihood1,
                                       lowerBound, upperBound)
        (minYsim2, maxYsim2, medianYsim2, meanYsim2) = \
            calculateUncertaintyBounds(self.ysim2, self.likelihood2,
                                       lowerBound, upperBound)
    
        # Are the distributions the same?
        if ks_stat:
            print("Calculating Kolmogorov-Smirnov statistic for weighted ensemble mean of daily streamflow...")
            self.calculateKolmogorovSmirnov(wghtMean1, wghtMean2)
    
        # Plot it up
        fig = plt.figure(figsize=(sizeX, sizeY), dpi=dpi, tight_layout=True)
        ax = fig.add_subplot(121)
        
        data_plt = []
        # Draw shaded uncertainty envelope
        ax.fill_between(self.x1, minYsim1, maxYsim1, linewidth=0,
                        color=fillColor1)
        ax.fill_between(self.x2, minYsim2, maxYsim2, linewidth=0,
                        color=fillColor2, alpha=opacity)

        # Draw ensemble median
        if plotMedian:
            (p, ) = ax.plot(self.x1, medianYsim1, color=median_color1, linestyle='solid')
            data_plt.append(p)
            
            (p, ) = ax.plot(self.x2, medianYsim2, color=median_color2, linestyle='solid')
            data_plt.append(p)
            
            if(legend_items):
                my_legend_items.append("Median %s" % (legend_items[0]) )
                my_legend_items.append("Median %s" % (legend_items[1]) )
        # Draw weighted ensemble mean
        if plotWeightedMean:
            (p, ) = ax.plot(self.x1, wghtMean1, color=weighted_mean_color1, linestyle='solid', linewidth=1.0)
            data_plt.append(p)
            
            (p, ) = ax.plot(self.x2, wghtMean2, color=weighted_mean_color2, linestyle='solid', linewidth=0.5)
            data_plt.append(p)
            
            if(legend_items):
                my_legend_items.append("Weighted ensemble mean %s" % (legend_items[0]) )
                my_legend_items.append("Weighted ensemble mean %s" % (legend_items[1]) )
        
        # Annotations
        # X-axis
        quarterly = matplotlib.dates.MonthLocator(interval=3)
        ax.xaxis.set_major_locator(quarterly)
        ax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter('%b-%Y') )
        # Rotate
        plt.setp( ax.xaxis.get_majorticklabels(), rotation=45 )
        plt.setp( ax.xaxis.get_majorticklabels(), fontsize=6 )
        
        # Y-axis
        plt.setp( ax.get_yticklabels(), fontsize=6 )
        
        if log:
            ax.set_yscale('log')
        
        if xlabel:
            ax.set_xlabel(xlabel)
        if ylabel:
            ax.set_ylabel(ylabel)
        if title:
            fig.suptitle(title, y=1.03)
    
        # Plot exceedance plot
        ax2 = fig.add_subplot(122)
        
        if plotMedian:
            x, y = exceedance_prob(medianYsim1)
            ax2.plot(x, y, color=median_color1, linestyle='solid')
            
            x, y = exceedance_prob(medianYsim2)
            ax2.plot(x, y, color=median_color2, linestyle='solid')
            
        if plotWeightedMean:
            x, y = exceedance_prob(wghtMean1)
            ax2.plot(x, y, color=weighted_mean_color1, linestyle='solid')
            
            x, y = exceedance_prob(wghtMean2)
            ax2.plot(x, y, color=weighted_mean_color2, linestyle='solid')
        
        x, y = exceedance_prob(minYsim1)
        ax2.plot(x, y, color=fillColor1, linestyle='dashed')
        
        x, y = exceedance_prob(minYsim2)
        ax2.plot(x, y, color=fillColor2, linestyle='dashed')
        
        x, y = exceedance_prob(maxYsim1)
        ax2.plot(x, y, color=fillColor1, linestyle='dashed')
        
        x, y = exceedance_prob(maxYsim2)
        ax2.plot(x, y, color=fillColor2, linestyle='dashed')
        
        # Annotations
        # X-axis
        formatter = FuncFormatter(to_percent)
        ax2.xaxis.set_major_formatter(formatter)
        ax2.set_xlabel('Exceedance probability (%)')
        
        # Y-axis
        if log:
            ax2.set_yscale('log')
        else:
            ax2.set_ylim( ax.get_ylim() )
        
        plt.setp( ax2.get_xticklabels(), fontsize=6 )
        plt.setp( ax2.get_yticklabels(), fontsize=6 )
    
        # Plot legend last
        if legend_items:
            legend = fig.legend( data_plt, my_legend_items, 'lower center', fontsize='x-small', 
                                ncol=len(my_legend_items), frameon=True )
            frame = legend.get_frame()
            frame.set_facecolor('0.25')
            frame.set_alpha(0.5)
            
        # Save the figure
        fig.savefig(plotFilepath, format=format, bbox_inches='tight', pad_inches=0.25)
    
    def main(self, args):
        # Set up command line options
        parser = argparse.ArgumentParser(description="Tool for making comparative visualizions of output from two behavioral model runs for RHESSys")
        parser.add_argument("basedirs", metavar="BASEDIRS", nargs=2,
                            help="Base directories of the two calibration sessions to be compared")
        
        parser.add_argument("postprocess_sessions", metavar="POSTPROC_SESSIONS", type=int, nargs=2,
                            help="Post-process session IDs of the two behavioral runs.")
        
        parser.add_argument("-t", "--title",
                            dest="title",
                            help="Title to add to output figures")
        
        parser.add_argument("-o", "--outdir", required=False, default=os.getcwd(),
                            dest="outdir",
                            help="Output directory in which to place figures.  If not specified, the currect directory will be used.")

        parser.add_argument("--enddate", type=int, nargs=4,
                            help="Date on which to end fitness calculationss, of format YYYY M D H")

        parser.add_argument("-of", "--outputFormat", action="store",
                            dest="outputFormat", default="PDF", choices=["PDF", "PNG"],
                            help="Output format to save figures in.")

        parser.add_argument("--behavioral_filter", action="store",
                            dest="behavioral_filter", required=False,
                            default="nse>0.5 and nse_log>0.5",
                            help="SQL where clause to use to determine which runs qualify as behavioral parameters.  E.g. 'nse>0.5 AND nse_log>0.5' (use quotes)")

        parser.add_argument("--supressObs", action="store_true", required=False, default=False,
                            help="Do not plot observered data")

        group = parser.add_mutually_exclusive_group()
        group.add_argument("--plotMedian", action="store_true", required=False, default=False,
                            help="Plot mean value of behavioral runs")
        group.add_argument("--plotWeightedMean", action="store_true", required=False, default=False,
                            help="Plot weighted ensemble mean value (Seibert and Beven 2009) of behavioral runs")

        parser.add_argument("--color", action="store_true", required=False, default=False,
                            help="Plot in color")
        
        parser.add_argument("--opacity", action="store", required=False, 
                            type=float, default=0.5,
                            help="Opacity used to draw shaded uncertainty region of second calibration session.")
        
        parser.add_argument("-l", "--legend", action="store", required=False, nargs=2,
                            dest="legend_items",
                            help="Legend items. If not supplied, no legend will be printed")
        
        parser.add_argument('--figureX', required=False, type=int, default=4,
                            help='The width of the plot, in inches')
    
        parser.add_argument('--figureY', required=False, type=int, default=3,
                            help='The height of the plot, in inches')

        parser.add_argument('--computeMassbalance', required=False, action='store_true', default=False,
                            help='Deteremine whether to compute mass balance statistics (e.g. Horton index)')

        parser.add_argument("--loglevel", action="store",
                            dest="loglevel", default="OFF",
                            help="Set logging level, one of: OFF [default], DEBUG, CRITICAL (case sensitive)")
        
        options = parser.parse_args()
        
        # Base of output filename
        behavioralFilename = 'behavioral_comparison'
        
        # Enforce initial command line options rules
        if "DEBUG" == options.loglevel:
            self._initLogger(logging.DEBUG)
        elif "CRITICAL" == options.loglevel:
            self._initLogger(logging.CRITICAL)
        else:
            self._initLogger(logging.NOTSET)
        
        basedir1 = options.basedirs[0]
        session1 = options.postprocess_sessions[0]
        if not os.path.isdir(basedir1) or not os.access(basedir1, os.R_OK):
            sys.exit("Unable to access basedir %s" % (basedir1,) )
        behavioralFilename += '_' + os.path.basename(basedir1)
        basedir1 = os.path.abspath(basedir1)
        
        basedir2 = options.basedirs[1]
        session2 = options.postprocess_sessions[1]
        if not os.path.isdir(basedir2) or not os.access(basedir2, os.R_OK):
            sys.exit("Unable to access basedir %s" % (basedir2,) )
        behavioralFilename += '_V_' + os.path.basename(basedir2)
        basedir2 = os.path.abspath(basedir2)
            
        if not os.path.isdir(options.outdir) and os.access(options.outdir, os.W_OK):
            parser.error("Figure output directory %s must be a writable directory" % (options.outdir,) )
        outdirPath = os.path.abspath(options.outdir)

        endDate = None
        if options.enddate:
            # Set end date based on command line
            endDate = datetime(options.enddate[0],
                               options.enddate[1],
                               options.enddate[2],
                               options.enddate[3])

        try:
            print("Using behavioral filter: {0}".format(options.behavioral_filter))
            # Read data for first behavioral run
            (runsProcessed1, self.obs1, self.x1, self.ysim1, self.likelihood1) = \
                self.readBehavioralData(basedir1, session1, 'streamflow',
                                        behavioral_filter=options.behavioral_filter,
                                        end_date=endDate)
            
            # Read data for second behavioral run
            (runsProcessed2, self.obs2, self.x2, self.ysim2, self.likelihood2) = \
                self.readBehavioralData(basedir2, session2, 'streamflow',
                                        behavioral_filter=options.behavioral_filter,
                                        end_date=endDate)

            if runsProcessed1 and runsProcessed2:
                if options.supressObs:
                    behavioralFilename += '_noObs'
                if options.plotMedian:
                    behavioralFilename += '_median'
                if options.plotWeightedMean:
                    behavioralFilename += '_wght_mean'
                if options.color:
                    behavioralFilename += '_color'
                if options.legend_items:
                    behavioralFilename += '_legend'
                # Generate visualizations
                self.saveUncertaintyBoundsComparisonPlot(outdirPath, behavioralFilename, 
                                               2.5, 97.5, format=options.outputFormat, log=False,
                                               ylabel=r'Streamflow ($mm^{-d}$)',
                                               title=options.title, 
                                               plotObs=(not options.supressObs),
                                               plotMedian=options.plotMedian,
                                               plotWeightedMean=options.plotWeightedMean,
                                               plotColor=options.color,
                                               legend_items=options.legend_items,
                                               sizeX=options.figureX, sizeY=options.figureY,
                                               opacity=options.opacity )
                behavioralFilename += '-log'
                self.saveUncertaintyBoundsComparisonPlot(outdirPath, behavioralFilename, 
                                               2.5, 97.5, format=options.outputFormat, log=True,
                                               ylabel=r'Streamflow ($mm^{-d}$)',
                                               title=options.title,
                                               plotObs=(not options.supressObs),
                                               plotMedian=options.plotMedian,
                                               plotWeightedMean=options.plotWeightedMean,
                                               plotColor=options.color,
                                               legend_items=options.legend_items,
                                               sizeX=options.figureX, sizeY=options.figureY,
                                               opacity=options.opacity, ks_stat=True )
                
                if options.computeMassbalance:
                    print("\nCalculating massbalance statistics...\n")
                    # Should refactor this as we are reading behavioral data twice
                    # Read data for first behavioral run
                    (runsProcessed1, obs1, x1, sim1, likelihood1) = \
                        self.readBehavioralDataMulti(basedir1, session1, ['streamflow', 'evap', 'trans', 'baseflow'],
                                                     behavioral_filter=options.behavioral_filter,
                                                     end_date=endDate)
                    sim_et1 = []    
                    hi1_avg = 0
                    for sim in sim1:
                        et = sim.evap + sim.trans
                        sim_et1.append(et)
                        total_et = et.sum()
                        total_baseflow = sim.baseflow.sum()
                        hi1_avg += total_et / (total_et + total_baseflow)
                    
                    wghtMeanET1 = calculateWeightedEnsembleMean(np.array(sim_et1), self.likelihood1)
                    yearlyWghtMeanET1 = wghtMeanET1.sum() / (len(wghtMeanET1) / 365)
                        
                    hi1_avg /= len(sim1)
                    
                    print("\nTotal weighted ensemble mean ET: %f" % (wghtMeanET1.sum(),) )
                    print("Yearly weighted ensemble mean ET: %f" % (yearlyWghtMeanET1,) )
                    print("Average Horton index: %f\n\n" % (hi1_avg,) )
                    
                    # Read data for second behavioral run
                    (runsProcessed2, obs2, x2, sim2, likelihood2) = \
                        self.readBehavioralDataMulti(basedir2, session2, ['streamflow', 'evap', 'trans', 'baseflow'],
                                                     behavioral_filter=options.behavioral_filter,
                                                     end_date=endDate)
                    sim_et2= []
                    hi2_avg = 0
                    for sim in sim2:
                        et = sim.evap + sim.trans
                        sim_et2.append(et)
                        total_et = et.sum()
                        total_baseflow = sim.baseflow.sum()
                        hi2_avg += total_et / (total_et + total_baseflow)
                        
                    wghtMeanET2 = calculateWeightedEnsembleMean(np.array(sim_et2), self.likelihood2)
                    yearlyWghtMeanET2 = wghtMeanET2.sum() / (len(wghtMeanET2) / 365)
                        
                    hi2_avg /= len(sim2)
                    
                    print("\nTotal weighted ensemble mean ET: %f" % (wghtMeanET2.sum(),) )
                    print("Yearly weighted ensemble mean ET: %f" % (yearlyWghtMeanET2,) )
                    print("Average Horton index: %f\n\n" % (hi2_avg,) )
                
            else:
                if not runsProcessed1:
                    errorStr = "Did not read any behavioral data for basedir %s, post-process session %d"
                    self.logger.error(errorStr % \
                                      (basedir1, session1) )
                if not runsProcessed2:
                    self.logger.error(errorStr % \
                                      (basedir2, session2) )
                return 1
        except:
            raise
        else:
            self.logger.debug("exiting normally")
            return 0
        finally:
            # Decrement reference count, this will (hopefully) allow __del__
            #  to be called on the once referenced object
            calibratorDB = None

class BehavioralTimeseriesOut(RHESSysCalibratorPostprocessBehavioral):
    
    TIMESERIES_EXT = os.extsep + 'csv'
    
    def main(self, args):
        # Set up command line options
        parser = argparse.ArgumentParser(description="Output timeseries from behavioral model runs of RHESSys")
        parser.add_argument("-b", "--basedir", action="store", 
                            dest="basedir", required=True,
                            help="Base directory for the calibration session")
        
        parser.add_argument("-s", "--postprocess_session", action="store", type=int,
                            dest="postprocess_id", required=True,
                            help="Post-process session to use for behavioral runs.")
        
        parser.add_argument("-o", "--outdir", action="store", required=False,
                            dest="outdir",
                            help="Output directory in which to place figures.  If not specified, basedir will be used.")

        parser.add_argument("-f", "--file", action="store", required=False, default='behavioral_ts',
                            dest="outfile",
                            help="The base name of the output timeseries. Files will be created for min, max, mean, and median timesers. Filenames will end in '.csv'")

        parser.add_argument("--lowerBound", required=False, type=float, default=2.5,
                            help="Percentile of lower bound of uncertainty range")

        parser.add_argument("--upperBound", required=False, type=float, default=97.5,
                            help="Percentile of upper bound of uncertainty range")

        parser.add_argument("--behavioral_filter", action="store",
                            dest="behavioral_filter", required=False,
                            default="nse>0.5 and nse_log>0.5",
                            help="SQL where clause to use to determine which runs qualify as behavioral parameters.  E.g. 'nse>0.5 AND nse_log>0.5' (use quotes)")

        parser.add_argument("-l", "--loglevel", action="store",
                            dest="loglevel", default="OFF",
                            help="Set logging level, one of: OFF [default], DEBUG, CRITICAL (case sensitive)")
        
        options = parser.parse_args()
        
        # Enforce initial command line options rules
        if "DEBUG" == options.loglevel:
            self._initLogger(logging.DEBUG)
        elif "CRITICAL" == options.loglevel:
            self._initLogger(logging.CRITICAL)
        else:
            self._initLogger(logging.NOTSET)
        
        if not os.path.isdir(options.basedir) or not os.access(options.basedir, os.W_OK):
            sys.exit("Unable to write to basedir %s" % (options.basedir,) )
        basedir = os.path.abspath(options.basedir)
        
        if not options.outdir:
            options.outdir = basedir
            
        if not os.path.isdir(options.outdir) and os.access(options.outdir, os.W_OK):
            parser.error("Figure output directory %s must be a writable directory" % (options.outdir,) )
        outdirPath = os.path.abspath(options.outdir)

        try:
            print("Using behavioral filter: {0}".format(options.behavioral_filter))
            (runsProcessed, self.obs, self.x, self.ysim, self.likelihood) = \
                self.readBehavioralData(basedir, options.postprocess_id, 'streamflow',
                                        observed_file=None, behavioral_filter=options.behavioral_filter)
            if runsProcessed:
                # Get the uncertainty boundary
                colName = 'streamflow'
                indexLabel = 'datetime'
                
                (minYsim, maxYsim, medianYsim, meanYsim) = \
                    calculateUncertaintyBounds(self.ysim, self.likelihood,
                                               options.lowerBound, options.upperBound)
                
                behavioralFilename = "%s_POSTPROCESS_SESSION_%s" % ( options.outfile, options.postprocess_id )
                behavioralFilepath = os.path.join( outdirPath, behavioralFilename )
                # Write out min timeseries
                minFilepath = behavioralFilepath + '_min' + BehavioralTimeseriesOut.TIMESERIES_EXT
                
                min_ts = pd.Series(minYsim, index=self.x, name=colName)
                min_ts.to_csv(minFilepath, header=True, index_label=indexLabel)
                
                # Write out max timeseries
                maxFilepath = behavioralFilepath + '_max' + BehavioralTimeseriesOut.TIMESERIES_EXT
                max_ts = pd.Series(maxYsim, index=self.x, name=colName)
                max_ts.to_csv(maxFilepath, header=True, index_label=indexLabel)
                
                # Write out median timeseries
                medianFilepath = behavioralFilepath + '_median' + BehavioralTimeseriesOut.TIMESERIES_EXT
                median_ts = pd.Series(medianYsim, index=self.x, name=colName)
                median_ts.to_csv(medianFilepath, header=True, index_label=indexLabel)
                
                # Write out mean timeseries
                meanFilepath = behavioralFilepath + '_mean' + BehavioralTimeseriesOut.TIMESERIES_EXT
                mean_ts = pd.Series(meanYsim, index=self.x, name=colName)
                mean_ts.to_csv(meanFilepath, header=True, index_label=indexLabel)
                
                # Write out weighted ensemble mean timeseries (Seibert and Beven 2009)
                wghtMeanFilepath = behavioralFilepath + '_weighted_ensmb_mean' + BehavioralTimeseriesOut.TIMESERIES_EXT
                wghtMean = calculateWeightedEnsembleMean(self.ysim, self.likelihood)
                mean_ts = pd.Series(wghtMean, index=self.x, name=colName)
                mean_ts.to_csv(wghtMeanFilepath, header=True, index_label=indexLabel)
                
        except:
            raise
        else:
            self.logger.debug("exiting normally")
            return 0
        finally:
            # Decrement reference count, this will (hopefully) allow __del__
            #  to be called on the once referenced object
            calibratorDB = None
    